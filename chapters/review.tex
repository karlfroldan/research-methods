%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Review of Related Systems and Related Literature}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Due to the trend of parallelization in the modern computing era, \cite{Rios2011,WeinstockHolladay,ZaghloulAlJami2017,Sanz2016}
several researchers developed more efficient and faster implementations 
of the A* algorithm by parallizing the algorithm in various ways.

\section{The A* Algorithm}
The A* pathfinding algorithm, first described by Hart, Nilsson, and Raphael, is a \emph{best-first search} algorithm 
for finding a path between two vertices in a directed weighted graph.\cite{HartNilssonRaphael1968}
The A* search algorithm can be seen as a variant of Dijkstra's algorithm with an added heuristic function to guide its search 
for an optimal solution. That is, the A* search functions the same as Dijkstra's algorithm if the heuristic function $h(n)=0$
for any $n$.\cite{Ferguson2005} 

Hart, Nilsson, and Raphael had proven that the A* algorithm is \emph{admissible} whenever the heuristic
function $h$ is admissibe. That is, A* is guaranteed to return an optimal solution.
The optimality of the path generated by the A* algorithm depends on its heuristic function which will be discussed further in 
Chapter 3 of this paper. 
% Here, we shall describe the original A* algorithm by Hart, Nilsson, and Raphael.\cite{HartNilssonRaphael1968}
% A pure-function pseudocode\ref{seqAStar} will be implemented instead of the usual imperative 
% pseudocode. But the pseudocode still follows the specifications of the original paper.

% \begin{algorithm}
%     \caption{Sequential A* Algorithm}
%     \label{seqAStar}
%     \begin{algorithmic}
%         \REQUIRE $G$ - a weighted labeled graph.
%         \REQUIRE $(\psi, \xi)$ - a tuple of two sets: the open set and the closed set
%         \REQUIRE $h$ - the heuristic function. $h(n)\neq 0$ or this equates to Dijkstra's Algorithm.
%         \REQUIRE $(\sigma, \gamma)$ - a tuple of two vertices in $G$: the start vertex and end vertex.
%         \ENSURE  $p$ - The shortest path from $\psi$ to $\xi$. 
        
%         \STATE let $S$ be a homogenous set of vertices in a graph.
%         \STATE astar :: Graph $\rightarrow$ ($S$, $S$) $\rightarrow$ ((Int, Int) 
%             $\rightarrow$ (Int, Int) $\rightarrow$ Int) $\rightarrow$ (Vertex, Vertex) $\rightarrow$ Path 
%         \STATE astar g s h se p = \textbf{WRITE THE ALGORITHM LATER}.
%     \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}[H]
% 	\begin{algorithmic}[1]
% 	\REQUIRE $R$ - set of strings
% 	\ENSURE superstring of set $R$
% 	\WHILE{||$R$|| > 1}
% 		\STATE choose $x_1 \neq x_2 \in R$ such that $overlap(x_1,x_2)$ is maximal 
% 		\STATE $R \leftarrow (R - \{x_1, x_2\}) \cup \{merge(x_1, x_2)\}$
% 	\ENDWHILE
% 	\RETURN remaining string in $R$
% 	\end{algorithmic}
% 	\caption{GREEDY(R)}
% 	\label{alg:seq}
% \end{algorithm}

\section{Parallel A* Algorithm}
Zaghloul, Al-Jami, Bakalla, et al. had shown that an implementation of a parallel A* algorithm 
had a speed up of 3-4 times on problem sizes of 2000x2000 to 4000x4000.\cite{ZaghloulAlJami2017}
The implementation took advantage of reducing the problem into subgraphs where the neighbors of a
vertex in a graph will have threads assigned to them and a neighbor $n_i$ will be the new start 
vertex of the A* search for that given thread with the same goal vertex. By Ahmdal's Law, 
runtime efficiency will decrease when the number of threads are increased due to the overhead 
of garbage collection and thread creation. 

\subsection{Bidirectional A* Search Algorithms}
Bidirectional search is said to be possible when there is exactly one start vertex and exactly one goal vertex.
Since the computer cannot differentiate between the two if they are reversible, it is possible to start 
searching from the goal $g$ to the start $s$ and vice versa.\cite{KainlKainz1997} Bidirectional search 
is useful when implemented on a unidirectional multigraph(\ref{uni-multigraph1})
where there can be an efficient path between from 
one vertex to another in one direction but the same path cannot exist in the other direction.

The BHPA algorithm is a bidirectional search employing two A* algorithms running sequentially with a sequential 
computer switching directions multiple times. The BHPA algorithm terminates whenever two search paths from both directions 
meet and returns the minimum $f$ cost of either direction or by adding the $g$ costs of the two directions.
However, Kaindl and Kainz had proven that the best case performance of BHPA* is not significantly better than 
the unidirectional A* algorithm due to the cost of the BHPA's termination condition.\cite{KainlKainz1997}
Kaindl and Kainz developed a more efficient implementation of a non-tradition bidirectional heuristic search that is effective 
on machines with limited memory. The algorithm instantiates by assigning memory to a unidirectional approach and uses A* to find 
a path from the start to the end vertices. If there is such a path given the memory, then the algorithm terminates. Otherwise, 
a reverse search from the end vertex to an intermediate vertex from the first search.

The New Bidirectional A* Search (NBA*) uses the same technique as the BHPA in the sense that the execution of each search direction alternates.\cite{Pijls2009}
However, NBA* employs the reverse search on a reverse graph. That is, let $V(G)$ and $E(G)$ be the vertex set and edge set of graph $G$ and if $u,v\in V(G)$ and 
$uv\in E(G)$ but $vu\notin E(G)$, then for the reverse graph $\text{rev}(G)$ with $u,v\in V(\text{rev}(G))$ and $vu\in E(\text{rev}(G))$ but $uv\notin E(\text{rev}(G))$.
The edges $uv$ in $E(G)$ and $vu$ in $E(\text{rev}(G))$ have the same weights. Likewise, if $s_1$ and $t_1$ are the start and goal vertices of 
graph $G$, then $s_2$ and $t_2$ are the start and goal vertices of graph $\text{rev}(G)$ with $s_2=t_1$ and $t_2=s_1$. As a corollary, the vertex set of $\text{rev}(G)$
is equal to the vertex set of $G$ denoted by $V$ from this point.

In NBA*, all vertices $v\in V$ are initially labeled $g(v)=\infty$ and the set $\mathcal{S}=\varnothing$ is the set of vertices with
permanent labels. Likewise, there are shared variables $\mathcal{L}=\infty$ and $\mathcal{R}=\varnothing$ which will hold the the 
best cost so far and the rejected vertices, respectively. During the runtime of the algorithm, a vertex is either expanded if its heuristics 
or label satisfies the necessary conditions with respect to $\mathcal{L}$ or rejected and thus added in the set $\mathcal{R}$.
The algorithm halts when there is one search side with no more candidates to be expanded.



    \begin{figure}
        \caption{A directional multigraph}
        \label{uni-multigraph1}
        \begin{center}
            \begin{tikzpicture}
                \begin{scope}[every node/.style={circle,thick,draw}]
                    \node (A) at (0,0) {A};
                    \node (B) at (4,0) {B};
                    \node (C) at (7,3) {C};
                    \node (D) at (7,-3) {D};
                    \node (E) at (10,0) {E};
                \end{scope}
                
                \begin{scope}[>={Stealth[black]},
                              every node/.style={fill=white,circle},
                              every edge/.style={draw=red,very thick}]
                    \path [->] (A) edge[bend left=30] node {$5$} (B);
                    \path [->] (B) edge[bend right=-30] node {$5$} (A);
                    
                    \path [->] (B) edge[bend left=30] node {$10$} (C);
                    \path [->] (C) edge[bend right=-30] node {$10$} (B);
                    \path [->] (D) edge node {$7$} (B);
    
                    \path [->] (E) edge[bend left=30] node {$3$} (D);
                    \path [->] (D) edge[bend right=-30] node {$4$} (E);
                    \path [->] (C) edge[bend left=30] node {$5$} (E);
                    \path [->] (E) edge[bend right=-30] node {$10$} (C);

                \end{scope}
            \end{tikzpicture} 
        \end{center}
        
    \end{figure}

\subsubsection{Parallel New Bidirectional A* (PNBA*)}
PNBA* was proposed by Rios and Chaimowicz by parallelizing the NBA* algorithm.\cite{Rios2011}
PNBA* leverages on a shared memory model and runs both search processes in parallel. Due to this, the PNBA* algorithm is 
faster than the NBA* and thus has a better runtime performance. The implementation of PNBA* has a variable $\mathcal{S}$, like the 
NBA*, shared by both processes and contains the vertices, say $v$, with finite labels $g(v)$. However, since both $\mathcal{L}$
and $\mathcal{S}$ are updated by both processes in parallel, it is possible that there would be a race condition and some vertex 
in one thread may be expanded due to the race conditions. However, this does not affect the admissibility of PNBA*.
While this may incur some overhead due to expanding additional vertices, parallelism makes up for the additional overhead costs.\cite{Rios2011}

\subsection{Parallelism by Hashing}
The Hash Distributed A* Algorithm (HDA*) builds on the the hash-based work distribution of PRA* and asynchronous communication
by TDA.\cite{Kishimoto2009,Evett1995,Romein1999} The HDA* search introduces \emph{ownership} where each processor has their own local 
closed and open lists and the search space is divided among processors. Unlike the implementation by Zaghloul, when a vertex is expanded 
by a processor $P_1$ in HDA*, $P_1$ will not immediately own the generated vertices but instead send those vertices to the message queue 
to be hashed by available processors which can then be processed. A pseudo-random hash function is important to make sure work is distributed 
equally among available processors. If the hash function is static, that is it only assigns every generated vertex to the same processor that 
generated said vertices, then the algorithms performs the same way as the classic A* algorithm. Due to the parallel nature of HDA* and having 
local open and closed sets per processor, there is no guarantee that an expanded vertex will no longer be expanded.

The simplest approach for the termination condition of HDA* is the \emph{barrier method} wherein a processor waits for other processors 
when its open set is empty. This incurs some overhead and is not viable. Weinstock and Holladay describes the \emph{sum flags method} where a 
binary flag is raised by a processor if its open set is empty.\cite{WeinstockHolladay} Let $p_i$ be processors for all $i\in {1..n}$ where $n$ is the number of processors 
in the computer. The algorithm will terminate if $p_1\land p_2\land\dots\land p_n=True$.

The original paper by Kishimoto, Fukunaga, and Botea showed that HDA* performs significantly better than A*,
WSA*\footnote{Another parallel implementation of the A* algorithm that involves work stealing}, and PRA* when using message passing 
on a number of problems. When the HDA* is run using 4 CPU cores, the speedup averages 3 times and is shown to be very efficient 
compared to the other parallel implementations and the sequential A* algorithm. Likewise, the speedup is even more significant at 
an average of 5 times when using 8 cores. Kishimoto, Fukunaga, and Botea expounded further that the HDA* is also efficient in a 
distributed computing environment by showing a speedup of an average of 30-60 times in a 128-core distributed computer.\cite{Kishimoto2009}

\section{Real-time Performance of Functional Programs}
The dominant programming languages used for time-critical software systems such as GPS systems and flight guidance softwares 
are written in mid-level programming languages that has direct hardware access such as C or C++. Frame and Coffey were concerned that 
the correctness of C and C++ were refutable and thus benchmarked and compared the real-time performance of the same 
program written in C++ and in Haskell, the latter being a programming language known to its correctness due to its strict typing rules.\cite{Frame2014}
Frame and Coffey hypothesized that imperative and functional programming may differ in their implementations but both are powerful enough to 
take on math-intensive problems.

Scott and Frame also tacked the ease of use of the programming language to the programmers and they concluded that 
the functional programming language may seem unfamiliar to an imperative language user at first but may be easier to 
understand and maintain in the long run.\cite{Frame2014} This is especially true in static typing languages such as Haskell 
where programmers will be able to make less typing errors such as adding a \verb|uint_8| to a \verb|bool| which may cause an overflow.\cite{Hanenberg2014}
The paper by Scott and Frame showed that functional programs are less verbose and are more expressive than their imperative counterparts 
for the same problem, however it still lacks in performance power. However, despite the slower runtime performance of functional programs, mathematically 
provable functional programs are still capable in being written as runtime verification systems in ultra-critical systems, as 
demonstrated by the National Aeronautics and Space Administration (NASA).\cite{NasaCopilot2020} Due to this, it is 
important to develop more performant, perhaps parallel, algorithms for functional programs especially in the area of 
combinatorial optimization, such as the problem posed in this paper.

Harris, Marlow, and Jones evaluated the parallel performance of Haskell and overhead costs when using parallel data structures.
Since Haskell is a garbage collected programming language, there would be some performance cost whenever data is being garbage collected.
However, due to the lazy-nature of Haskell, unecessary data that will never be used in any computation will not be evaluated.
The parallel impleentation of the Glasgow Haskell Compiler (GHC) does not really state explicit parallelism. Instead, a programmer may 
annotate (called \emph{sparking}) whether a computation can be parallelized or not with different strategies. 
It is up to the Haskell runtime system whether the annotation will be assigned to a different processor or thread. 
This abstraction enables the programmer to be worry-free about how parallelism will be implemented and diverts it to 
other matters such as performance and how many sparks will be created.\cite{Harris2005,Marlow2009} 
% Several researches have provided in depth study on approximation
% algorithms for the SCS problem.

% Most \cite{Turner1989, Ma2009, Zaritsky2004}, however, considered a greedy algorithm to solve the SCS problem.
% The reason why greed works for shortest common superstring problem was
% explored by \cite{Ma2009}. They explained the good performance 
% of the greedy algorithm by using the smoothed analysis. 
% That is, for any  given instance $I$ of $SCS$, 
% the average approximation ratio of the 
% greedy algorithm on a small random perturbation of $I$ is $1+o(1)$. 

% Apart from the greedy approach, other methods including bio-inspired
% algorithms, have also been employed to solve lots of hard problems in 
% computer science including the SCS problem. 
% In \cite{Zaritsky2004}'s work, for example, four approaches for 
% finding solutions to the SCS problem: a 
% standard genetic algorithm, a novel cooperative-coevolutionary 
% algorithm, a benchmark greedy algorithm, and a parallel 
% coevolutionary-greedy approach have been compared. In the paper,
% the coevolutionary approach produced the best results.

% Zaritsky et al's \cite{Zaritsky2004} work served as the 
% foundation of this paper. Hence, the following algorithms,
% which have been explored and tested in the latter, have
% been extracted and validated 
% whether they are indeed generating
% good approximate solution to the SCS problem.

% \subsection{Greedy-SCS}

% The core idea of the greedy algorithm is to repeatedly merge
%  pairs of distinct strings with maximum overlap until only one remains.
%  It has been conjectured by \cite{MaierStorer1777} that the superstring
%  produced by the greedy algorithm is at most two times the optimal.
% Below is a pseudocode for the greedy algorithm:

% \begin{algorithm}[H]
% 	\begin{algorithmic}[1]
% 	\REQUIRE $R$ - set of strings
% 	\ENSURE superstring of set $R$
% 	\WHILE{||$R$|| > 1}
% 		\STATE choose $x_1 \neq x_2 \in R$ such that $overlap(x_1,x_2)$ is maximal 
% 		\STATE $R \leftarrow (R - \{x_1, x_2\}) \cup \{merge(x_1, x_2)\}$
% 	\ENDWHILE
% 	\RETURN remaining string in $R$
% 	\end{algorithmic}
% 	\caption{GREEDY(R)}
% 	\label{alg:seq}
% \end{algorithm}

% The greedy algorithm is probably the most widely used
% heuristic used in DNA-sequencing due to its simplicity and 
% constant performance guarantee which is most likely to be factor-2.

% \subsection{Genetic Algorithm}

% In this paper, we follow the standard model for the genetic algorithm (see Algorithm \ref{alg:gga}).
% That is, the algorithm generates an initial population of 
% random candidate solutions, then 
% the process of selection based on a defined fitness function,
% crossover, and mutation are used to evolve the next generation, of which
% each individual is then evaluated and
% assigned a fitness value. These steps are repeated a
% predefined number of times or until the solution is
% satisfactory. 


% \begin{algorithm}[H]
% 	\begin{algorithmic}[1]
% 	\STATE $t \leftarrow 0$ \{generation number\} 
% 	\STATE initialize Population$_{t}$ 
% 	\STATE evaluate(Population$_{t}$)
% 	\WHILE{termination condition not met}
% 		\STATE select individuals from Population$_{t}$ 
% 		\STATE recombine individuals
% 		\STATE mutate individuals	
% 		\STATE Population$_{t+1} \leftarrow $ newly created individuals
% 		\STATE $t \leftarrow t + 1$
% 		\STATE evaluate(Population$_{t}$) 			
% 	\ENDWHILE
% 	\RETURN solution derived from the best individual in Population$_{t}$
% 	\end{algorithmic}
% 	\caption{GENERIC GA()}
% 	\label{alg:gga}
% \end{algorithm}
