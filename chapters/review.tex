%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Review of Related Systems and Related Literature}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Due to the trend of parallelization in the modern computing era, \cite{Rios2011,WeinstockHolladay,ZaghloulAlJami2017,Sanz2016}
several researchers developed more efficient and faster implementations 
of the A* algorithm by parallizing the algorithm in various ways.

\section{The A* Algorithm}
Here, we shall describe the original A* algorithm by Hart, Nilsson, and Raphael.\cite{HartNilssonRaphael1968}
A pure-function pseudocode\ref{seqAStar} will be implemented instead of the usual imperative 
pseudocode. But the pseudocode still follows the specifications of the original paper.

\begin{algorithm}
    \caption{Sequential A* Algorithm}
    \label{seqAStar}
    \begin{algorithmic}
        \REQUIRE $G$ - a weighted labeled graph.
        \REQUIRE $(\psi, \xi)$ - a tuple of two sets: the open set and the closed set
        \REQUIRE $h$ - the heuristic function. $h(n)\neq 0$ or this equates to Dijkstra's Algorithm.
        \REQUIRE $(\sigma, \gamma)$ - a tuple of two vertices in $G$: the start vertex and end vertex.
        \ENSURE  $p$ - The shortest path from $\psi$ to $\xi$. 
        
        \STATE let $S$ be a homogenous set of vertices in a graph.
        \STATE astar :: Graph $\rightarrow$ ($S$, $S$) $\rightarrow$ ((Int, Int) 
            $\rightarrow$ (Int, Int) $\rightarrow$ Int) $\rightarrow$ (Vertex, Vertex) $\rightarrow$ Path 
        \STATE astar g s h se p = \textbf{WRITE THE ALGORITHM LATER}.
    \end{algorithmic}
\end{algorithm}

% \begin{algorithm}[H]
% 	\begin{algorithmic}[1]
% 	\REQUIRE $R$ - set of strings
% 	\ENSURE superstring of set $R$
% 	\WHILE{||$R$|| > 1}
% 		\STATE choose $x_1 \neq x_2 \in R$ such that $overlap(x_1,x_2)$ is maximal 
% 		\STATE $R \leftarrow (R - \{x_1, x_2\}) \cup \{merge(x_1, x_2)\}$
% 	\ENDWHILE
% 	\RETURN remaining string in $R$
% 	\end{algorithmic}
% 	\caption{GREEDY(R)}
% 	\label{alg:seq}
% \end{algorithm}

\section{Parallel A* Algorithm}
Here, we shall discuss the naive implementation of Zaghloul\cite{ZaghloulAlJami2017}
where the researchers parallelized A* using MISD and compared the running times and speed up 
of the algorithm in solving some problems based ont he number of cores of the computer it was run on.

Also, we shall visit the implementations by Rios and Chaomowicz\cite{Rios2011}
which used a more enhanced implementation of the single-core bidirectional pathfinding algorithm by 
Kaindl and Kainz\cite{KainlKainz1997} by parallelizing the two directions on two CPU cores.

Likewise, we shall also visit HDA* by Kishimoto, Fukunaga, and Botea\cite{Kishimoto2009} on how it 
assigned ownership of vertices between different CPU cores by hashing and the performance comparison of
Holladay and Weinstock\cite{WeinstockHolladay}.

Lastly, we compare how the HDA* may be a problem for Haskell since 
Haskell's parallelism is abstracted. Instead, the paper will define 
another method of parallelizing the A* algorithm on a multicore machine 
using Haskell.

\section{Inductive Graphs}
This section will discuss the \verb|functional graph library| as described by Erwig.\cite{Erwig2001}
Inductive graphs can be constructed monadically which will be of advantage when writing 
parallelizing an algorithm since parallel code can only be executed monadically.

We use both inductive graphs for the sequential implementation (not a monad) and for the parallel
implementation (monad).

\section{Real-time Performance of Functional Programs}
In this section, we compare the usefulness of functional programs in real-time situations, that is, in 
applications were time is critical such as GPS systems and flight guidance software.\cite{Frame2014}
Since we're primarily concerned with developing a fast-enough parallel program for a correct software that 
would be needed for critical systems, we have to compare the performance of our program with recent imperative 
implementations. This should be stated here.

Likewise, we make use of Tim Harris, Simon Marlow, and Simon Peyton Jones' research on the parallel 
performance of Haskell and overhead costs when using parallel data structures.\cite{Harris2005}


% Several researches have provided in depth study on approximation
% algorithms for the SCS problem.

% Most \cite{Turner1989, Ma2009, Zaritsky2004}, however, considered a greedy algorithm to solve the SCS problem.
% The reason why greed works for shortest common superstring problem was
% explored by \cite{Ma2009}. They explained the good performance 
% of the greedy algorithm by using the smoothed analysis. 
% That is, for any  given instance $I$ of $SCS$, 
% the average approximation ratio of the 
% greedy algorithm on a small random perturbation of $I$ is $1+o(1)$. 

% Apart from the greedy approach, other methods including bio-inspired
% algorithms, have also been employed to solve lots of hard problems in 
% computer science including the SCS problem. 
% In \cite{Zaritsky2004}'s work, for example, four approaches for 
% finding solutions to the SCS problem: a 
% standard genetic algorithm, a novel cooperative-coevolutionary 
% algorithm, a benchmark greedy algorithm, and a parallel 
% coevolutionary-greedy approach have been compared. In the paper,
% the coevolutionary approach produced the best results.

% Zaritsky et al's \cite{Zaritsky2004} work served as the 
% foundation of this paper. Hence, the following algorithms,
% which have been explored and tested in the latter, have
% been extracted and validated 
% whether they are indeed generating
% good approximate solution to the SCS problem.

% \subsection{Greedy-SCS}

% The core idea of the greedy algorithm is to repeatedly merge
%  pairs of distinct strings with maximum overlap until only one remains.
%  It has been conjectured by \cite{MaierStorer1777} that the superstring
%  produced by the greedy algorithm is at most two times the optimal.
% Below is a pseudocode for the greedy algorithm:

% \begin{algorithm}[H]
% 	\begin{algorithmic}[1]
% 	\REQUIRE $R$ - set of strings
% 	\ENSURE superstring of set $R$
% 	\WHILE{||$R$|| > 1}
% 		\STATE choose $x_1 \neq x_2 \in R$ such that $overlap(x_1,x_2)$ is maximal 
% 		\STATE $R \leftarrow (R - \{x_1, x_2\}) \cup \{merge(x_1, x_2)\}$
% 	\ENDWHILE
% 	\RETURN remaining string in $R$
% 	\end{algorithmic}
% 	\caption{GREEDY(R)}
% 	\label{alg:seq}
% \end{algorithm}

% The greedy algorithm is probably the most widely used
% heuristic used in DNA-sequencing due to its simplicity and 
% constant performance guarantee which is most likely to be factor-2.

% \subsection{Genetic Algorithm}

% In this paper, we follow the standard model for the genetic algorithm (see Algorithm \ref{alg:gga}).
% That is, the algorithm generates an initial population of 
% random candidate solutions, then 
% the process of selection based on a defined fitness function,
% crossover, and mutation are used to evolve the next generation, of which
% each individual is then evaluated and
% assigned a fitness value. These steps are repeated a
% predefined number of times or until the solution is
% satisfactory. 


% \begin{algorithm}[H]
% 	\begin{algorithmic}[1]
% 	\STATE $t \leftarrow 0$ \{generation number\} 
% 	\STATE initialize Population$_{t}$ 
% 	\STATE evaluate(Population$_{t}$)
% 	\WHILE{termination condition not met}
% 		\STATE select individuals from Population$_{t}$ 
% 		\STATE recombine individuals
% 		\STATE mutate individuals	
% 		\STATE Population$_{t+1} \leftarrow $ newly created individuals
% 		\STATE $t \leftarrow t + 1$
% 		\STATE evaluate(Population$_{t}$) 			
% 	\ENDWHILE
% 	\RETURN solution derived from the best individual in Population$_{t}$
% 	\end{algorithmic}
% 	\caption{GENERIC GA()}
% 	\label{alg:gga}
% \end{algorithm}
